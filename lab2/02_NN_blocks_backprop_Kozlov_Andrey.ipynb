{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PqC4R7SGseKa"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "## 2.1 Создание нейронов и полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "2.1.1. Используя операции над матрицами и векторами из библиотеки `torch`, реализовать нейрон с заданными весами `weights` и `bias`. Прогнать вектор `inputs` через нейрон и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результат: tensor(1.4901e-08)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        # Создаем тензоры для весов и смещения\n",
    "        self.weights = torch.tensor(weights)\n",
    "        self.bias = torch.tensor(bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Умножаем вход на веса, добавляем смещение и применяем активацию (например, сигмоиду)\n",
    "        result = torch.matmul(self.weights, inputs) + self.bias\n",
    "        return result\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    weights = [0.5, -0.5]\n",
    "    bias = 0.2\n",
    "    neuron = Neuron(weights, bias)\n",
    "    inputs = torch.tensor([0.3, 0.7])\n",
    "    output = neuron.forward(inputs)\n",
    "\n",
    "    print(\"Результат:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7])\n",
    "bias = 3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qJvnwiyty37"
   },
   "source": [
    "2.1.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой с заданными весами `weights` и `biases`. Прогнать вектор `inputs` через слой и вывести результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fVWF3a9vtx90"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11824\\1185308104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3.14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mlinearlayer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinearlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Linear() takes no arguments"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class Linear:\n",
    "    def init(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return torch.matmul(inputs, self.weights) + self.biases\n",
    "    \n",
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0]).reshape(1, -1)\n",
    "weights = torch.tensor([-0.2, 0.3, -0.5, 0.7, 0.1, -0.4, 0.6, -0.8, 0.3, -0.5, 0.7, -0.9])\n",
    "biases = torch.tensor([3.14, 1.5, -2.2])\n",
    "\n",
    "linearlayer = Linear(weights, biases)\n",
    "output = linearlayer.forward(inputs)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Fo-JFnHPuFCS"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "weights = torch.tensor([[-0.2, 0.3, -0.5, 0.7],\n",
    "                        [0.5, -0.91, 0.26, -0.5],\n",
    "                        [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "biases = torch.tensor([3.14, 2.71, 7.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQtsJzcxuyGd"
   },
   "source": [
    "2.1.3 Реализовать полносвязный слой из __2.1.2__ таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. Продемонстрировать работу.\n",
    "Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z8IizmtsuhO1"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "2.1.4 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения). Прогнать вектор `inputs` через слой и вывести результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IOv52EdovASs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.0417,   3.8162,  -9.0545,   1.7390,  -8.5381,  -2.0492,   1.7480],\n",
      "        [  5.6258,  -0.2470,  -4.1200, -10.0819,  -3.3205,   1.0583,   0.8020],\n",
      "        [  3.7543,   2.0330,  -5.6896,   1.9437,  -0.1713,   0.9863,  -1.1333]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, n_features, n_neurons):\n",
    "        # Инициализация весов из стандартного нормального распределения\n",
    "        self.weights = nn.Parameter(torch.randn(n_features, n_neurons))\n",
    "        # Инициализация смещений (biases) нулями\n",
    "        self.biases = nn.Parameter(torch.zeros(n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Прогоняем входные данные через слой\n",
    "        # Для этого умножаем входные данные на веса, затем добавляем смещения\n",
    "        output = torch.matmul(inputs, self.weights) + self.biases\n",
    "        return output\n",
    "\n",
    "# Создаем экземпляр слоя с 7 нейронами и 4 входами\n",
    "n_neurons = 7\n",
    "n_features = 4\n",
    "linear_layer = Linear(n_features, n_neurons)\n",
    "\n",
    "# Создаем вектор inputs\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Прогоняем inputs через слой\n",
    "output = linear_layer.forward(inputs)\n",
    "\n",
    "# Выводим результат\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "2.1.5 Используя решение из __2.1.4__, создать 2 полносвязных слоя и пропустить матрицу `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выбрать произвольно, количество нейронов во втором слое выбрать так, чтобы результатом прогона являлась матрица (3x7). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RjjQIQlTxJE6"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.0196e-01,  1.2120e-01, -2.3489e-01,  2.1246e-01, -2.2733e-02,\n",
      "          2.8719e-01,  2.2201e-01],\n",
      "        [-7.5407e-01,  2.4780e-01, -6.6660e-04,  1.2916e-01, -1.1580e-01,\n",
      "          4.2921e-01,  7.8224e-01],\n",
      "        [-4.7808e-01,  4.4714e-01, -5.2780e-01,  1.0357e-01,  2.6123e-01,\n",
      "          1.1584e+00, -2.5004e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Создаем матрицу inputs\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5],\n",
    "                       [2, 5, -1, 2],\n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Определяем класс модели\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=4, out_features=10)  # Первый полносвязный слой\n",
    "        self.fc2 = nn.Linear(in_features=10, out_features=7)  # Второй полносвязный слой\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))  # Применяем ReLU к первому слою\n",
    "        x = self.fc2(x)  # Второй слой без нелинейности\n",
    "        return x\n",
    "\n",
    "# Создаем экземпляр модели\n",
    "model = MyModel()\n",
    "\n",
    "# Пропускаем inputs через модель\n",
    "output = model(inputs)\n",
    "\n",
    "# Выводим результат\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "## 2.2 Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2.2.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.7246, 0.4549],\n",
      "        [0.0000, 0.7455, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.2473, 0.4409, 0.6254]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Применяем функцию активации ReLU\n",
    "        return torch.relu(inputs)\n",
    "\n",
    "# Создаем матрицу размера (4, 3) со случайными числами из стандартного нормального распределения\n",
    "matrix = torch.randn(4, 3)\n",
    "relu_activation = ReLU()\n",
    "result = relu_activation.forward(matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "2.2.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации softmax:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6d7500d980c313da83e4117da701bf7c8f1982f5)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5859, 0.1149, 0.2992],\n",
      "        [0.1005, 0.1384, 0.7611],\n",
      "        [0.8251, 0.0685, 0.1065],\n",
      "        [0.2220, 0.4820, 0.2960]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Применяем функцию активации Softmax\n",
    "        return F.softmax(inputs, dim=-1)\n",
    "\n",
    "# Создаем матрицу размера (4, 3) со случайными числами из стандартного нормального распределения\n",
    "matrix = torch.randn(4, 3)\n",
    "softmax_activation = Softmax()\n",
    "result = softmax_activation.forward(matrix)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "2.2.3 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию активации ELU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/eb23becd37c3602c4838e53f532163279192e4fd)\n",
    "\n",
    "Создать матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверить работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NzMz7HDLySxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9554, -0.9370, -0.4796],\n",
      "        [-0.6708, -0.8166, -0.1695],\n",
      "        [-0.0393,  0.6921,  0.9509],\n",
      "        [ 1.3038, -0.7398,  1.3778]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class ELU:\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Применяем функцию активации ELU\n",
    "        return torch.where(inputs > 0, inputs, self.alpha * (torch.exp(inputs) - 1))\n",
    "\n",
    "# Создаем матрицу размера (4, 3) со случайными числами из стандартного нормального распределения\n",
    "matrix = torch.randn(4, 3)\n",
    "elu_activation = ELU(alpha=1.0)  # Здесь alpha - параметр ELU\n",
    "result = elu_activation.forward(matrix)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "## 2.3 Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "2.3.1 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "\n",
    "Создать полносвязный слой с 1 нейроном, прогнать через него батч `inputs` и посчитать значение MSE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.325974464416504\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MSELoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Рассчитываем среднеквадратичную ошибку\n",
    "        mse = torch.mean((y_pred - y_true) ** 2)\n",
    "        return mse\n",
    "\n",
    "# Создаем полносвязный слой с 1 нейроном\n",
    "linear_layer = torch.nn.Linear(4, 1)\n",
    "\n",
    "# Входные данные\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Истинные значения\n",
    "y = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
    "\n",
    "# Прогоняем входные данные через слой\n",
    "y_pred = linear_layer(inputs).squeeze()\n",
    "\n",
    "# Создаем экземпляр функции потерь MSE и вычисляем потерю\n",
    "mse_loss = MSELoss()\n",
    "loss = mse_loss.forward(y_pred, y)\n",
    "\n",
    "print(loss.item())  # Выводим значение MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "y = torch.tensor([2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "2.3.2 Используя операции над матрицами и векторами из библиотеки `torch`, реализовать функцию потерь Categorical Cross-Entropy:\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "Создать полносвязный слой с 3 нейронами и прогнать через него батч `inputs`. Полученный результат пропустить через функцию активации softmax. Посчитать значение CCE, трактуя вектор `y` как вектор правильных ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.153682231903076\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CategoricalCrossentropyLoss:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Применяем Softmax к предсказаниям\n",
    "        y_pred_softmax = F.softmax(y_pred, dim=-1)\n",
    "        \n",
    "        # Рассчитываем потерю CCE\n",
    "        cce_loss = -torch.sum(y_true * torch.log(y_pred_softmax))\n",
    "        return cce_loss\n",
    "\n",
    "# Создаем полносвязный слой с 3 нейронами\n",
    "linear_layer = torch.nn.Linear(4, 3)\n",
    "\n",
    "# Входные данные\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Истинные метки (one-hot encoding)\n",
    "y = torch.tensor([[0, 1, 0],\n",
    "                  [1, 0, 0],\n",
    "                  [1, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "# Прогоняем входные данные через слой\n",
    "y_pred = linear_layer(inputs)\n",
    "\n",
    "# Создаем экземпляр функции потерь CCE и вычисляем потерю\n",
    "cce_loss = CategoricalCrossentropyLoss()\n",
    "loss = cce_loss.forward(y_pred, y)\n",
    "\n",
    "print(loss.item())  # Выводим значение CCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7Qoupfo1ZGJ"
   },
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                        [2, 5, -1, 2], \n",
    "                        [-1.5, 2.7, 3.3, -0.8]])\n",
    "y = torch.tensor([1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "2.3.3 Модифицировать 2.3.1, добавив L2-регуляризацию.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d92ca2429275bfdc0474523babbafe014ca8b580)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1336112022399902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class MSELossL2:\n",
    "    def __init__(self, lambda_):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def data_loss(self, y_pred, y_true):\n",
    "        # Рассчитываем среднеквадратичную ошибку (MSE)\n",
    "        mse = torch.mean((y_pred - y_true) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def reg_loss(self, layer):\n",
    "        # Рассчитываем L2-регуляризацию для слоя\n",
    "        l2_reg = 0.0\n",
    "        for param in layer.parameters():\n",
    "            l2_reg += torch.sum(param ** 2)\n",
    "        return 0.5 * self.lambda_ * l2_reg\n",
    "\n",
    "    def forward(self, y_pred, y_true, layer):\n",
    "        data_loss = self.data_loss(y_pred, y_true)\n",
    "        reg_loss = self.reg_loss(layer)\n",
    "        return data_loss + reg_loss\n",
    "\n",
    "# Создаем полносвязный слой с 1 нейроном\n",
    "linear_layer = torch.nn.Linear(4, 1)\n",
    "\n",
    "# Входные данные\n",
    "inputs = torch.tensor([[1, 2, 3, 2.5], \n",
    "                       [2, 5, -1, 2], \n",
    "                       [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# Истинные значения\n",
    "y = torch.tensor([2, 3, 4], dtype=torch.float32)\n",
    "\n",
    "# Прогоняем входные данные через слой\n",
    "y_pred = linear_layer(inputs).squeeze()\n",
    "\n",
    "# Создаем экземпляр функции потерь MSE с L2-регуляризацией и вычисляем потерю\n",
    "mse_loss_l2 = MSELossL2(lambda_=0.01)  # Здесь lambda - коэффициент регуляризации\n",
    "loss = mse_loss_l2.forward(y_pred, y, linear_layer)\n",
    "\n",
    "print(loss.item())  # Выводим значение MSE с L2-регуляризацией\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w049ZSdR6qQi"
   },
   "source": [
    "## 2.4 Обратное распространение ошибки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBtCfSME9W7Q"
   },
   "source": [
    "2.4.1 Используя один нейрон и SGD (1 пример за шаг), решите задачу регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4xmI-QJ66WAF"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_features=4, n_informative=4, coef=True, bias=0.5)\n",
    "X = torch.from_numpy(X).to(torch.float32)\n",
    "y = torch.from_numpy(y).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpPSPYSpD9Ey"
   },
   "source": [
    "[Граф вычислений для этой задачи](https://i.ibb.co/2dhDxZx/photo-2021-02-15-17-18-04.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc1sXtGd_J-y"
   },
   "source": [
    "2.4.1.1 Реализуйте класс `SquaredLoss`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llFigkqd_JRU"
   },
   "outputs": [],
   "source": [
    "class SquaredLoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return # <реализовать логику MSE>\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dinput = # df/dc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GY7ForfM97UQ"
   },
   "source": [
    "2.4.1.2. Модифицируйте класс `Neuron` из __2.1.1__:\n",
    "\n",
    "  1) Сделайте так, чтобы веса нейрона инициализировались из стандартного нормального распределения\n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "L0KqxPJU9kAN"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    self.b = torch.randn(1)\n",
    "    self.w = torch.randn(n_features)\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    y_pred = torch.dot(inputs, self.w)+self.b\n",
    "    return  y_pred\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss\n",
    "    \n",
    "    self.dw = dvalue*self.inputs # df/dW\n",
    "    self.db = dvalue*1 # dE/b = de/dy_pred * dy_pred/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKcO4zOLACxM"
   },
   "source": [
    "2.4.1.3 Допишите цикл для настройки весов нейрона\n",
    "\n",
    "[SGD](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%BE%D1%85%D0%B0%D1%81%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA)\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_g_FvwvmALJd"
   },
   "outputs": [],
   "source": [
    "n_inputs = X.size()[1]# <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 10 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(100):\n",
    "  for x_example, y_example in zip(X, y):\n",
    "    # forward pass\n",
    "    y_pred = neuron.forward(x_example)\n",
    "    curr_loss = loss.forward(y_pred, y_example)\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    loss.backward(y_pred, y_example)\n",
    "    neuron.backward(loss.dvalue)\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>\n",
    "    neuron.w -= learning_rate*neuron.dw\n",
    "    neuron.b -= learning_rate*neuron.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebibge9VEgF7"
   },
   "source": [
    "2.4.2 Решите задачу 2.4.1, используя пакетный градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "as-QeWSdOELd"
   },
   "source": [
    "Вычисления для этой задачи: \n",
    "[1](https://i.ibb.co/rmtQT6P/photo-2021-02-15-18-00-43.jpg)\n",
    "[2](https://i.ibb.co/NmCFVnQ/photo-2021-02-15-18-01-17.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr9qq4H_J3zt"
   },
   "source": [
    "2.4.1.1 Модифицируйте класс `MSELoss` из __2.3.1__, реализовав расчет производной относительно предыдущего слоя с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "L8wjk9iPMQ4x"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "  def forward(self, y_pred, y_true):\n",
    "    return (y_pred-y_true)**2\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    self.dvalue = 2*(y_pred-y_true)   # здесь интегрируются атрибуты класса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3fSHCEtJjX8"
   },
   "source": [
    "2.4.2.2. Модифицируйте класс `Neuron` из __2.4.1.2__:\n",
    "\n",
    "  1) Реализуйте метод `forward` таким образом, чтобы он мог принимать на вход матрицу (батч) с данными. \n",
    "\n",
    "  2) Реализуйте расчет градиента относительно весов `weights` и `bias` с учетом того, что теперь работа ведется с батчами, а не с индивидуальными примерами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_OpuAP0Jpz1"
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "  def __init__(self, n_inputs):\n",
    "    # <создать атрибуты объекта weights и bias>\n",
    "    self.b = torch.randn(1)\n",
    "    self.w = to rch.randn(n_features)\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    y_pred = torch.dot(inputs, self.w)+self.b\n",
    "    return # <реализовать логику нейрона>\n",
    "  \n",
    "  def backward(self, dvalue):\n",
    "    # dvalue - значение градиента, которое приходит нейрону от следующего слоя сети\n",
    "    # в данном случае это будет градиент L по y^ (созданный методом backwards у объекта MSELoss\n",
    "    \n",
    "    self.dweights = dvalue*self.inputs # df/dW\n",
    "    self.dbias = dvalue*1 # dE/b = de/dy_pred * dy_pred/db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zO-NZrgKMBFx"
   },
   "source": [
    "2.4.2.3 Допишите цикл для настройки весов нейрона"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zqwm_7eqJim1"
   },
   "outputs": [],
   "source": [
    "n_inputs = # <размерность элемента выборки >\n",
    "learning_rate = 0.1 #  скорость обучения\n",
    "n_epoch = 100 #  количество эпох\n",
    "\n",
    "neuron = Neuron(n_inputs)\n",
    "loss = MSELoss()\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    y_pred = # <прогон через нейрон>\n",
    "    curr_loss = # <прогон через функцию потерь>\n",
    "    losses.append(curr_loss)\n",
    "\n",
    "    # backprop\n",
    "    # <вызов методов backward>\n",
    "    # обратите внимание на последовательность вызовов: от конца к началу\n",
    "\n",
    "    # <шаг оптимизации для весов (weights и bias) нейрона>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16VtP159OdMk"
   },
   "source": [
    "2.4.3  Используя один полносвязный слой и  пакетный градиетный спуск, решите задачу регрессии из __2.4.1__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uj5febreSSZ7"
   },
   "source": [
    "2.4.3.1 Модифицируйте класс `Linear` из __2.1.4__. ([вычисление градиентов](https://i.ibb.co/kgVR6m6/photo-2021-02-15-21-30-28.jpg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zWuhaLdSB2_"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, n_features, n_neurons):\n",
    "    # <создать атрибуты объекта weights и biases>\n",
    "    pass\n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    return # <реализовать логику слоя>\n",
    "\n",
    "  def backward(self, dvalues):\n",
    "    self.dweights = # df/dW\n",
    "    self.dbiases = # df/db\n",
    "    self.dinputs = # df/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3w1hT9MS_Lt"
   },
   "source": [
    "2.4.3.2 Создайте слой с одним нейроном. Используя класс MSELoss из 2.4.2, убедитесь, что модель обучается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTkJV-F8TVuN"
   },
   "source": [
    "2.4.4 Используя наработки из 2.4, создайте нейросеть и решите задачу регрессии.\n",
    "\n",
    "Предлагаемая архитектура: \n",
    "1. Полносвязный слой с 10 нейронами\n",
    "2. Активация ReLU\n",
    "3. Полносвязный слой с 1 нейроном"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axUjpPz-SvS1"
   },
   "outputs": [],
   "source": [
    "X = torch.linspace(-1, 1, 100).view(-1, 1)\n",
    "y = X.pow(2) + 0.2 * torch.rand(X.size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXoiNxkpTziV"
   },
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = inputs.clip(min=0)\n",
    "    return self.output\n",
    "  \n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues.clone()\n",
    "    self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXhspwW6T44T"
   },
   "outputs": [],
   "source": [
    "# создание компонентов сети\n",
    "# fc1 = \n",
    "# relu1 = \n",
    "# fc2 = \n",
    "\n",
    "loss = MSELoss()\n",
    "lr = 0.02\n",
    "\n",
    "ys = []\n",
    "for epoch in range(2001):\n",
    "  # <forward pass>\n",
    "  # fc1 > relu1 > fc2 > loss\n",
    "\n",
    "  data_loss = # <прогон через функцию потерь>\n",
    "\n",
    "  if epoch % 200 == 0:\n",
    "    print(f'epoch {epoch} mean loss {data_loss}')\n",
    "    ys.append(out)\n",
    "  \n",
    "  # <backprop> \n",
    "  # loss > fc2 > relu1 > fc1\n",
    "\n",
    "  # <шаг оптимизации для fc1>\n",
    "\n",
    "  # <шаг оптимизации для fc2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpKi0OfoUkwk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(len(ys), 1, figsize=(10, 40))\n",
    "for ax, y_ in zip(axs, ys):\n",
    "  ax.scatter(X.numpy(), y.numpy(), color = \"orange\")\n",
    "  ax.plot(X.numpy(), y_.numpy(), 'g-', lw=3)\n",
    "  ax.set_xlim(-1.05, 1.5)\n",
    "  ax.set_ylim(-0.25, 1.25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDgJRHjuyArfKO8ZT68MsS",
   "name": "02_NN_blocks_backprop_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
